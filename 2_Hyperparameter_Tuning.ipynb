{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Stage 2: Hyperparameter Tuning with Optuna\n",
                "\n",
                "## Innovation Feature 5: Automated Hyperparameter Optimization\n",
                "\n",
                "This notebook optimizes **DenseNet121, Xception, and ResNet50** using Optuna.\n",
                "\n",
                "**Requirements:**\n",
                "- ‚ö° **T4 GPU** enabled (Runtime ‚Üí Change runtime type ‚Üí T4 GPU)\n",
                "- üì¶ Upload `enhanced_dataset.zip` from Stage 1\n",
                "- ‚è±Ô∏è Expected runtime: **2-4 hours**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required libraries\n",
                "!pip install optuna tensorflow -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers, models\n",
                "from tensorflow.keras.applications import DenseNet121, Xception, ResNet50\n",
                "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
                "import optuna\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "import os\n",
                "import zipfile\n",
                "from pathlib import Path\n",
                "\n",
                "# Check GPU\n",
                "print('TensorFlow version:', tf.__version__)\n",
                "print('GPU available:', tf.config.list_physical_devices('GPU'))\n",
                "if not tf.config.list_physical_devices('GPU'):\n",
                "    print('‚ö†Ô∏è  WARNING: No GPU detected! Enable T4 GPU in Runtime ‚Üí Change runtime type')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Extract Enhanced Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract enhanced dataset\n",
                "dataset_zip = 'enhanced_dataset.zip'\n",
                "dataset_folder = 'enhanced_dataset'\n",
                "\n",
                "if os.path.exists(dataset_zip):\n",
                "    print('Extracting enhanced dataset...')\n",
                "    with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
                "        zip_ref.extractall('.')\n",
                "    print(f'‚úì Extracted to {dataset_folder}/')\n",
                "elif os.path.exists(dataset_folder):\n",
                "    print(f'‚úì Dataset folder already exists')\n",
                "else:\n",
                "    print('ERROR: Please upload enhanced_dataset.zip from Stage 1!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check dataset structure\n",
                "classes = sorted([d for d in os.listdir(dataset_folder) if os.path.isdir(os.path.join(dataset_folder, d))])\n",
                "print(f'Found {len(classes)} classes: {classes}')\n",
                "\n",
                "# Count images per class\n",
                "for cls in classes:\n",
                "    cls_path = os.path.join(dataset_folder, cls)\n",
                "    count = len([f for f in os.listdir(cls_path) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
                "    print(f'  {cls}: {count} images')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Preparation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hyperparameters\n",
                "IMG_SIZE = 224\n",
                "NUM_CLASSES = len(classes)\n",
                "SEED = 42\n",
                "\n",
                "# Data augmentation for training\n",
                "data_augmentation = keras.Sequential([\n",
                "    layers.RandomFlip('horizontal'),\n",
                "    layers.RandomRotation(0.1),\n",
                "    layers.RandomZoom(0.1),\n",
                "])\n",
                "\n",
                "# Create datasets with 70/15/15 split\n",
                "def create_datasets(batch_size=32):\n",
                "    # Load full dataset\n",
                "    full_ds = tf.keras.utils.image_dataset_from_directory(\n",
                "        dataset_folder,\n",
                "        image_size=(IMG_SIZE, IMG_SIZE),\n",
                "        batch_size=batch_size,\n",
                "        seed=SEED,\n",
                "        shuffle=True\n",
                "    )\n",
                "    \n",
                "    # Calculate split sizes\n",
                "    total_batches = tf.data.experimental.cardinality(full_ds).numpy()\n",
                "    train_size = int(0.7 * total_batches)\n",
                "    val_size = int(0.15 * total_batches)\n",
                "    \n",
                "    # Split dataset\n",
                "    train_ds = full_ds.take(train_size)\n",
                "    remaining = full_ds.skip(train_size)\n",
                "    val_ds = remaining.take(val_size)\n",
                "    test_ds = remaining.skip(val_size)\n",
                "    \n",
                "    # Apply augmentation to training set\n",
                "    train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
                "    \n",
                "    # Normalize all datasets\n",
                "    normalization_layer = layers.Rescaling(1./255)\n",
                "    train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
                "    val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
                "    test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))\n",
                "    \n",
                "    # Optimize performance\n",
                "    AUTOTUNE = tf.data.AUTOTUNE\n",
                "    train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
                "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
                "    test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
                "    \n",
                "    return train_ds, val_ds, test_ds\n",
                "\n",
                "print('Dataset creation function ready')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Building Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_model(model_name, dropout_rate=0.3):\n",
                "    \"\"\"\n",
                "    Create transfer learning model\n",
                "    \n",
                "    Args:\n",
                "        model_name: 'densenet', 'xception', or 'resnet'\n",
                "        dropout_rate: Dropout probability\n",
                "    \"\"\"\n",
                "    # Load base model (pre-trained on ImageNet)\n",
                "    if model_name == 'densenet':\n",
                "        base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
                "    elif model_name == 'xception':\n",
                "        base_model = Xception(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
                "    elif model_name == 'resnet':\n",
                "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
                "    else:\n",
                "        raise ValueError(f'Unknown model: {model_name}')\n",
                "    \n",
                "    # Freeze base model layers\n",
                "    base_model.trainable = False\n",
                "    \n",
                "    # Build full model\n",
                "    inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
                "    x = base_model(inputs, training=False)\n",
                "    x = layers.GlobalAveragePooling2D()(x)\n",
                "    x = layers.Dropout(dropout_rate)(x)\n",
                "    x = layers.Dense(256, activation='relu')(x)\n",
                "    x = layers.Dropout(dropout_rate)(x)\n",
                "    outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
                "    \n",
                "    model = keras.Model(inputs, outputs)\n",
                "    return model\n",
                "\n",
                "print('Model building function ready')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Optuna Objective Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def objective(trial, model_name):\n",
                "    \"\"\"\n",
                "    Optuna objective function for hyperparameter tuning\n",
                "    \"\"\"\n",
                "    # Clear session to avoid memory issues\n",
                "    keras.backend.clear_session()\n",
                "    \n",
                "    # Suggest hyperparameters\n",
                "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
                "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.6)\n",
                "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
                "    optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'nadam', 'rmsprop'])\n",
                "    \n",
                "    # Create datasets with suggested batch size\n",
                "    train_ds, val_ds, _ = create_datasets(batch_size)\n",
                "    \n",
                "    # Create model\n",
                "    model = create_model(model_name, dropout_rate)\n",
                "    \n",
                "    # Select optimizer\n",
                "    if optimizer_name == 'adam':\n",
                "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
                "    elif optimizer_name == 'nadam':\n",
                "        optimizer = keras.optimizers.Nadam(learning_rate=learning_rate)\n",
                "    else:\n",
                "        optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
                "    \n",
                "    # Compile model\n",
                "    model.compile(\n",
                "        optimizer=optimizer,\n",
                "        loss='sparse_categorical_crossentropy',\n",
                "        metrics=['accuracy']\n",
                "    )\n",
                "    \n",
                "    # Callbacks\n",
                "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
                "    \n",
                "    # Train model (limited epochs for speed)\n",
                "    history = model.fit(\n",
                "        train_ds,\n",
                "        validation_data=val_ds,\n",
                "        epochs=10,  # Limited epochs for Optuna trials\n",
                "        callbacks=[early_stop],\n",
                "        verbose=0\n",
                "    )\n",
                "    \n",
                "    # Return best validation accuracy\n",
                "    best_val_acc = max(history.history['val_accuracy'])\n",
                "    return best_val_acc\n",
                "\n",
                "print('Optuna objective function ready')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Optimize Each Model\n",
                "\n",
                "### 5.1 DenseNet121 Optimization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('='*50)\n",
                "print('OPTIMIZING DENSENET121')\n",
                "print('='*50)\n",
                "\n",
                "# Create Optuna study\n",
                "study_densenet = optuna.create_study(direction='maximize', study_name='densenet121_tuning')\n",
                "\n",
                "# Run optimization (20 trials)\n",
                "study_densenet.optimize(lambda trial: objective(trial, 'densenet'), n_trials=20)\n",
                "\n",
                "# Print results\n",
                "print('\\n' + '='*50)\n",
                "print('DENSENET121 - BEST HYPERPARAMETERS')\n",
                "print('='*50)\n",
                "print(f'Best validation accuracy: {study_densenet.best_value:.4f}')\n",
                "print('Best hyperparameters:')\n",
                "for key, value in study_densenet.best_params.items():\n",
                "    print(f'  {key}: {value}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 Xception Optimization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('='*50)\n",
                "print('OPTIMIZING XCEPTION')\n",
                "print('='*50)\n",
                "\n",
                "study_xception = optuna.create_study(direction='maximize', study_name='xception_tuning')\n",
                "study_xception.optimize(lambda trial: objective(trial, 'xception'), n_trials=20)\n",
                "\n",
                "print('\\n' + '='*50)\n",
                "print('XCEPTION - BEST HYPERPARAMETERS')\n",
                "print('='*50)\n",
                "print(f'Best validation accuracy: {study_xception.best_value:.4f}')\n",
                "print('Best hyperparameters:')\n",
                "for key, value in study_xception.best_params.items():\n",
                "    print(f'  {key}: {value}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.3 ResNet50 Optimization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('='*50)\n",
                "print('OPTIMIZING RESNET50')\n",
                "print('='*50)\n",
                "\n",
                "study_resnet = optuna.create_study(direction='maximize', study_name='resnet50_tuning')\n",
                "study_resnet.optimize(lambda trial: objective(trial, 'resnet'), n_trials=20)\n",
                "\n",
                "print('\\n' + '='*50)\n",
                "print('RESNET50 - BEST HYPERPARAMETERS')\n",
                "print('='*50)\n",
                "print(f'Best validation accuracy: {study_resnet.best_value:.4f}')\n",
                "print('Best hyperparameters:')\n",
                "for key, value in study_resnet.best_params.items():\n",
                "    print(f'  {key}: {value}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Train Final Models with Best Hyperparameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_final_model(model_name, best_params, epochs=30):\n",
                "    \"\"\"\n",
                "    Train model with optimized hyperparameters\n",
                "    \"\"\"\n",
                "    keras.backend.clear_session()\n",
                "    \n",
                "    # Create datasets\n",
                "    train_ds, val_ds, test_ds = create_datasets(best_params['batch_size'])\n",
                "    \n",
                "    # Create model\n",
                "    model = create_model(model_name, best_params['dropout_rate'])\n",
                "    \n",
                "    # Optimizer\n",
                "    if best_params['optimizer'] == 'adam':\n",
                "        optimizer = keras.optimizers.Adam(learning_rate=best_params['learning_rate'])\n",
                "    elif best_params['optimizer'] == 'nadam':\n",
                "        optimizer = keras.optimizers.Nadam(learning_rate=best_params['learning_rate'])\n",
                "    else:\n",
                "        optimizer = keras.optimizers.RMSprop(learning_rate=best_params['learning_rate'])\n",
                "    \n",
                "    # Compile\n",
                "    model.compile(\n",
                "        optimizer=optimizer,\n",
                "        loss='sparse_categorical_crossentropy',\n",
                "        metrics=['accuracy']\n",
                "    )\n",
                "    \n",
                "    # Callbacks\n",
                "    callbacks = [\n",
                "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
                "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7)\n",
                "    ]\n",
                "    \n",
                "    # Train\n",
                "    print(f'\\nTraining {model_name.upper()} for {epochs} epochs...')\n",
                "    history = model.fit(\n",
                "        train_ds,\n",
                "        validation_data=val_ds,\n",
                "        epochs=epochs,\n",
                "        callbacks=callbacks,\n",
                "        verbose=1\n",
                "    )\n",
                "    \n",
                "    # Evaluate on test set\n",
                "    test_loss, test_acc = model.evaluate(test_ds, verbose=0)\n",
                "    print(f'{model_name.upper()} - Test Accuracy: {test_acc:.4f}')\n",
                "    \n",
                "    # Save model\n",
                "    model_path = f'{model_name}_optimized.h5'\n",
                "    model.save(model_path)\n",
                "    print(f'‚úì Saved to {model_path}')\n",
                "    \n",
                "    return model, history, test_acc"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train all 3 models with best hyperparameters\n",
                "results = {}\n",
                "\n",
                "models_to_train = [\n",
                "    ('densenet', study_densenet.best_params),\n",
                "    ('xception', study_xception.best_params),\n",
                "    ('resnet', study_resnet.best_params)\n",
                "]\n",
                "\n",
                "for model_name, best_params in models_to_train:\n",
                "    model, history, test_acc = train_final_model(model_name, best_params, epochs=30)\n",
                "    results[model_name] = {\n",
                "        'model': model,\n",
                "        'history': history,\n",
                "        'test_acc': test_acc,\n",
                "        'best_params': best_params\n",
                "    }\n",
                "\n",
                "print('\\n' + '='*50)\n",
                "print('ALL MODELS TRAINED!')\n",
                "print('='*50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Results Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create comparison table\n",
                "comparison_data = []\n",
                "for model_name, result in results.items():\n",
                "    comparison_data.append({\n",
                "        'Model': model_name.upper(),\n",
                "        'Test Accuracy': f\"{result['test_acc']:.4f}\",\n",
                "        'Learning Rate': f\"{result['best_params']['learning_rate']:.2e}\",\n",
                "        'Dropout': result['best_params']['dropout_rate'],\n",
                "        'Batch Size': result['best_params']['batch_size'],\n",
                "        'Optimizer': result['best_params']['optimizer']\n",
                "    })\n",
                "\n",
                "df_results = pd.DataFrame(comparison_data)\n",
                "print('\\n' + '='*80)\n",
                "print('FINAL MODEL COMPARISON')\n",
                "print('='*80)\n",
                "print(df_results.to_string(index=False))\n",
                "print('='*80)\n",
                "\n",
                "# Save to CSV\n",
                "df_results.to_csv('model_comparison.csv', index=False)\n",
                "print('\\n‚úì Saved comparison to model_comparison.csv')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training histories\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "for idx, (model_name, result) in enumerate(results.items()):\n",
                "    history = result['history']\n",
                "    \n",
                "    axes[idx].plot(history.history['accuracy'], label='Train Accuracy')\n",
                "    axes[idx].plot(history.history['val_accuracy'], label='Val Accuracy')\n",
                "    axes[idx].set_title(f'{model_name.upper()}\\nTest Acc: {result[\"test_acc\"]:.4f}')\n",
                "    axes[idx].set_xlabel('Epoch')\n",
                "    axes[idx].set_ylabel('Accuracy')\n",
                "    axes[idx].legend()\n",
                "    axes[idx].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "print('‚úì Saved plot to training_curves.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Download Optimized Models\n",
                "\n",
                "Download these model files for Stage 3 (Ensemble):"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('Models saved:')\n",
                "for model_name in ['densenet', 'xception', 'resnet']:\n",
                "    model_file = f'{model_name}_optimized.h5'\n",
                "    if os.path.exists(model_file):\n",
                "        size_mb = os.path.getsize(model_file) / (1024*1024)\n",
                "        print(f'  ‚úì {model_file} ({size_mb:.1f} MB)')\n",
                "\n",
                "print('\\nDownload these .h5 files from the Files panel for Stage 3!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "‚úÖ **Completed:**\n",
                "- Optuna hyperparameter optimization (20 trials per model)\n",
                "- Trained 3 optimized models (DenseNet121, Xception, ResNet50)\n",
                "- Saved model weights (.h5 files)\n",
                "- Generated performance comparison\n",
                "\n",
                "**Next Step:** Use these 3 models in **Stage 3: Ensemble Stacking**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        },
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "accelerator": "GPU"
    },
    "nbformat": 4,
    "nbformat_minor": 0
}